# Web Scraping

## What is Web Scraping?
Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web. Whether you are a data scientist,
engineer, or anybody who analyzes large amounts of datasets, the ability to scrape data from the web is a useful skill to have. Let's say you find data from the web,
and there is no direct way to download it, web scraping using Python is a skill you can use to extract the data into a useful form that can be imported.

## Web Scraping using Beautiful Soup
Using Jupyter Notebook, you should start by importing the necessary modules (pandas, numpy, matplotlib.pyplot, seaborn). If you don't have Jupyter Notebook installed,
I recommend installing it using the Anaconda Python distribution which is available on the internet.
To easily display the plots, make sure to include the line %matplotlib inline as shown below.

### Web Scraping Techniques
- Human copy-and-paste
- Text pattern matching
- HTTP programming
- HTML parsing
- DOM parsing
- Vertical aggregation
- Semantic annotation recognizing
- Computer vision web-page analysis


### The web data scraping process

If you do it yourself using website scraping tools
This is what a general DIY web scraping process looks like:

### Identify the target website

Collect URLs of the pages where you want to extract data from
Make a request to these URLs to get the HTML of the page
Use locators to find the data in the HTML
Save the data in a JSON or CSV file or some other structured format
Simple enough, right? It is! If you just have a small project. But unfortunately, there are quite a few challenges you need to tackle if you need data at scale. For example, maintaining the scraper if the website layout changes, managing proxies, executing javascript, or working around antibots. These are all deeply technical problems that can eat up a lot of resources. There are multiple open-source web data scraping tools that you can use but they all have their limitations. That’s part of the reason many businesses choose to outsource their web data projects.

### If you outsource it

1. Our team gathers your requirements regarding your project.

2. Our veteran team of web data scraping experts writes the scraper(s) and sets up the infrastructure to collect your data and structure it based on your requirements.

3. Finally, we deliver the data in your desired format and desired frequency.


###  The software do we use to scrape the web?
By Scripting functions that can be used to extract & transform content as well as database interfaces that can store the scraped data in local databases.


## How do you find out if a website has blocked or banned you?
### 1. IP Rotation
The number one way sites detect web scrapers is by examining their IP address, thus most of web scraping without getting blocked is using a number of different IP addresses to avoid any one IP address from getting banned. To avoid sending all of your requests through the same IP address, you can use an IP rotation service like ScraperAPI or other proxy services in order to route your 
requests through a series of different IP addresses. This will allow you to scrape the majority of websites without issue.

### 2. Set a Real User Agent
User Agents are a special type of HTTP header that will tell the website you are visiting exactly what browser you are using. Some websites will examine User Agents and block requests from User Agents that don’t belong to a major browser.
Most web scrapers don’t bother setting the User Agent, and are therefore easily detected by checking for missing User Agents. 

### 3. Set Other Request Headers
Real web browsers will have a whole host of headers set, any of which can be checked by careful websites to block your web scraper.

### 4. Set Random Intervals In Between Your Requests
It is easy to detect a web scraper that sends exactly one request each second 24 hours a day! No real person would ever use a website like that, and an obvious pattern like this is easily detectable. Use randomized delays (anywhere between 2-10 seconds for example) in order to build a web scraper that can avoid being blocked. Also, remember to be polite, if you send requests too fast you can crash the website for everyone, if you detect that your requests are getting slower and slower, 
you may want to send requests more slowly so you don’t overload the web server (you’ll definitely want to do this to help frameworks like Scrapy avoid being banned).

### 5. Set a Referrer
The Referer header is an http request header that lets the site know what site you are arriving from.
Generally it’s a good idea to set this so that it looks like you’re arriving from Google, you can do this with the header

### 6. Use a Headless Browser
The trickiest websites to scrape may detect subtle tells like web fonts, extensions, browser cookies, and javascript execution in order to determine whether or not the request is coming from a real user.
In order to scrape these websites you may need to deploy your own headless browser (or have ScraperAPI do it for you!).
